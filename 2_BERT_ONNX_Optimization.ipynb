{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2 : Post-Optimization of BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Free-up the cuda space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             total       used       free     shared    buffers     cached\r\n",
      "Mem:         61401      15695      45705        515        990      11174\r\n",
      "-/+ buffers/cache:       3530      57870\r\n",
      "Swap:            0          0          0\r\n"
     ]
    }
   ],
   "source": [
    "!free -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 : Downloading the official scripts of Post-optimization of BERT using TF2ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-26 01:40:06--  https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/bert_model_optimization.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7165 (7.0K) [text/plain]\n",
      "Saving to: ‘./bert_op_scripts/bert_model_optimization.py’\n",
      "\n",
      "./bert_op_scripts/b 100%[===================>]   7.00K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-02-26 01:40:06 (79.6 MB/s) - ‘./bert_op_scripts/bert_model_optimization.py’ saved [7165/7165]\n",
      "\n",
      "--2020-02-26 01:40:06--  https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/BertOnnxModelTF.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26114 (26K) [text/plain]\n",
      "Saving to: ‘./bert_op_scripts/BertOnnxModelTF.py’\n",
      "\n",
      "./bert_op_scripts/B 100%[===================>]  25.50K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2020-02-26 01:40:07 (26.9 MB/s) - ‘./bert_op_scripts/BertOnnxModelTF.py’ saved [26114/26114]\n",
      "\n",
      "--2020-02-26 01:40:07--  https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/BertOnnxModel.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42679 (42K) [text/plain]\n",
      "Saving to: ‘./bert_op_scripts/BertOnnxModel.py’\n",
      "\n",
      "./bert_op_scripts/B 100%[===================>]  41.68K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2020-02-26 01:40:07 (23.9 MB/s) - ‘./bert_op_scripts/BertOnnxModel.py’ saved [42679/42679]\n",
      "\n",
      "--2020-02-26 01:40:07--  https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/OnnxModel.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.200.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.200.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22412 (22K) [text/plain]\n",
      "Saving to: ‘./bert_op_scripts/OnnxModel.py’\n",
      "\n",
      "./bert_op_scripts/O 100%[===================>]  21.89K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2020-02-26 01:40:07 (26.5 MB/s) - ‘./bert_op_scripts/OnnxModel.py’ saved [22412/22412]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir bert_op_scripts\n",
    "!wget -O ./bert_op_scripts/bert_model_optimization.py https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/bert_model_optimization.py\n",
    "!wget -O ./bert_op_scripts/BertOnnxModelTF.py https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/BertOnnxModelTF.py\n",
    "!wget -O ./bert_op_scripts/BertOnnxModel.py https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/BertOnnxModel.py\n",
    "!wget -O ./bert_op_scripts/OnnxModel.py https://raw.githubusercontent.com/microsoft/onnxruntime/master/onnxruntime/python/tools/bert/OnnxModel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\t\t out\t\t\t     uncased_L-12_H-768_A-12\r\n",
      "bert_op_scripts  squad-1.1\t\t     uncased_L-12_H-768_A-12.zip\r\n",
      "lost+found\t tensor-bert-pipeline.ipynb  Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp out/bert.onnx bert_op_scripts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3 : Analysing the script to make neccessary changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------------------------------------------------------\r\n",
      "# Copyright (c) Microsoft Corporation.  All rights reserved.\r\n",
      "# Licensed under the MIT License.\r\n",
      "#--------------------------------------------------------------------------\r\n",
      "\r\n",
      "# Convert Bert ONNX model converted from TensorFlow or exported from PyTorch to use Attention, Gelu,\r\n",
      "# SkipLayerNormalization and EmbedLayerNormalization ops to optimize\r\n",
      "# performance on NVidia GPU and CPU.\r\n",
      "\r\n",
      "# For Bert model exported from PyTorch, OnnxRuntime has bert model optimization support internally.\r\n",
      "# You can use the option --use_onnxruntime to use model optimization from OnnxRuntime package.\r\n",
      "# For Bert model file like name.onnx, optimized model for GPU or CPU from OnnxRuntime will output as\r\n",
      "# name_ort_gpu.onnx or name_ort_cpu.onnx in the same directory.\r\n",
      "# This script is retained for experiment purpose. Useful senarios like the following:\r\n",
      "#  (1) Change model from fp32 to fp16.\r\n",
      "#  (2) Change input data type from int64 to int32.\r\n",
      "#  (3) Some model cannot be handled by OnnxRuntime, and you can modify this script to get optimized model.\r\n",
      "\r\n",
      "# This script has been tested using the following models:\r\n",
      "#  (1) BertForSequenceClassification as in https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\r\n",
      "#      PyTorch 1.2 or above, and exported to Onnx using opset version 10 or 11.\r\n",
      "#  (2) BertForQuestionAnswering as in https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\r\n",
      "#      PyTorch 1.2 or above, and exported to Onnx using opset version 10 or 11.\r\n",
      "\r\n",
      "import logging\r\n",
      "import onnx\r\n",
      "import os\r\n",
      "import sys\r\n",
      "import argparse\r\n",
      "import numpy as np\r\n",
      "from collections import deque\r\n",
      "from onnx import ModelProto, TensorProto, numpy_helper\r\n",
      "import onnxruntime\r\n",
      "from BertOnnxModel import BertOnnxModel\r\n",
      "from BertOnnxModelTF import BertOnnxModelTF\r\n",
      "from BertOnnxModelKeras import BertOnnxModelKeras\r\n",
      "\r\n",
      "logger = logging.getLogger('')\r\n",
      "\r\n",
      "# Map model type to tuple: optimizer class, export tools (pytorch, tf2onnx, keras2onnx) and whether OnnxRuntime has the optimization.\r\n",
      "MODEL_CLASSES = {\r\n",
      "    \"bert\" : (BertOnnxModel, \"pytorch\", True),\r\n",
      "    \"bert_tf\": (BertOnnxModelTF, \"tf2onnx\", False),\r\n",
      "    \"bert_keras\" : (BertOnnxModelKeras, \"keras2onnx\", False)\r\n",
      "}\r\n",
      "\r\n",
      "def optimize_by_onnxruntime(onnx_model_path, use_gpu, optimized_model_path=None):\r\n",
      "    \"\"\"\r\n",
      "    Use onnxruntime package to optimize model. It could support models exported by PyTorch.\r\n",
      "\r\n",
      "    Args:\r\n",
      "        onnx_model_path (str): th path of input onnx model.\r\n",
      "        use_gpu (bool): whether the optimized model is targeted to run in GPU.\r\n",
      "        optimized_model_path (str or None): the path of optimized model.\r\n",
      "\r\n",
      "    Returns:\r\n",
      "        optimized_model_path: the path of optimized model\r\n",
      "    \"\"\"\r\n",
      "    if use_gpu and 'CUDAExecutionProvider' not in onnxruntime.get_available_providers():\r\n",
      "        logger.error(\"There is no gpu for onnxruntime to do optimization.\")\r\n",
      "        return onnx_model_path\r\n",
      "\r\n",
      "    sess_options = onnxruntime.SessionOptions()\r\n",
      "    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\r\n",
      "\r\n",
      "    if optimized_model_path is None:\r\n",
      "        path_prefix = onnx_model_path[:-5]   #remove .onnx suffix\r\n",
      "        optimized_model_path = \"{}_ort_{}.onnx\".format(path_prefix, \"gpu\" if use_gpu else \"cpu\")\r\n",
      "\r\n",
      "    sess_options.optimized_model_filepath = optimized_model_path\r\n",
      "\r\n",
      "    if not use_gpu:\r\n",
      "        session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=['CPUExecutionProvider'])\r\n",
      "    else:\r\n",
      "        session = onnxruntime.InferenceSession(onnx_model_path, sess_options)\r\n",
      "        assert 'CUDAExecutionProvider' in session.get_providers() # Make sure there is GPU\r\n",
      "\r\n",
      "    assert os.path.exists(optimized_model_path) and os.path.isfile(optimized_model_path)\r\n",
      "    logger.info(\"Save optimized model by onnxruntime to {}\".format(optimized_model_path))\r\n",
      "    return optimized_model_path\r\n",
      "\r\n",
      "def parse_arguments():\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    parser.add_argument('--input', required=True, type=str,\r\n",
      "                        help=\"input onnx model path\")\r\n",
      "\r\n",
      "    parser.add_argument('--output', required=True, type=str,\r\n",
      "                        help=\"optimized onnx model path\")\r\n",
      "\r\n",
      "    parser.add_argument('--model_type', required=False, type=str.lower, default=\"bert\",\r\n",
      "                        choices=list(MODEL_CLASSES.keys()),\r\n",
      "                        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\r\n",
      "\r\n",
      "    parser.add_argument('--num_heads', required=False, type=int, default=12,\r\n",
      "                        help=\"number of attention heads\")\r\n",
      "\r\n",
      "    parser.add_argument('--hidden_size', required=False, type=int, default=768,\r\n",
      "                        help=\"bert model hidden size. 768 for base model and 1024 for large\")\r\n",
      "\r\n",
      "    parser.add_argument('--sequence_length', required=False, type=int, default=128,\r\n",
      "                        help=\"max sequence length\")\r\n",
      "\r\n",
      "    parser.add_argument('--input_int32', required=False, action='store_true',\r\n",
      "                         help=\"Use int32 (instead of int64) tensor as input to avoid unnecessary data cast\")\r\n",
      "    parser.set_defaults(input_int32=False)\r\n",
      "\r\n",
      "    parser.add_argument('--float16', required=False, action='store_true',\r\n",
      "                        help=\"If your target device is V100 or T4 GPU, use this to convert float32 to float16 for best performance\")\r\n",
      "    parser.set_defaults(float16=False)\r\n",
      "\r\n",
      "    parser.add_argument('--gpu_only', required=False, action='store_true',\r\n",
      "                        help=\"whether the target device is gpu or not\")\r\n",
      "    parser.set_defaults(gpu_only=False)\r\n",
      "\r\n",
      "    parser.add_argument('--verbose', required=False, action='store_true')\r\n",
      "    parser.set_defaults(verbose=False)\r\n",
      "\r\n",
      "    args = parser.parse_args()\r\n",
      "\r\n",
      "    return args\r\n",
      "\r\n",
      "def optimize_model(input, model_type, gpu_only, num_heads, hidden_size, sequence_length, input_int32, float16):\r\n",
      "    (optimizer_class, framework, run_onnxruntime) = MODEL_CLASSES[model_type]\r\n",
      "\r\n",
      "    input_model_path = input\r\n",
      "    if run_onnxruntime:\r\n",
      "        input_model_path = optimize_by_onnxruntime(input_model_path, gpu_only)\r\n",
      "        logger.info(\"Use OnnxRuntime to optimize and save the optimized model to {}\".format(input_model_path))\r\n",
      "\r\n",
      "    model = ModelProto()\r\n",
      "    with open(input_model_path, \"rb\") as f:\r\n",
      "        model.ParseFromString(f.read())\r\n",
      "\r\n",
      "    bert_model = optimizer_class(model, num_heads, hidden_size, sequence_length, input_int32, float16, gpu_only)\r\n",
      "    bert_model.optimize()\r\n",
      "\r\n",
      "    return bert_model\r\n",
      "\r\n",
      "def main():\r\n",
      "    args = parse_arguments()\r\n",
      "\r\n",
      "    # output logging to stdout\r\n",
      "    log_handler = logging.StreamHandler(sys.stdout)\r\n",
      "    if args.verbose:\r\n",
      "        log_handler.setFormatter(logging.Formatter('[%(filename)s:%(lineno)s - %(funcName)20s()] %(message)s'))\r\n",
      "        logging_level = logging.DEBUG\r\n",
      "    else:\r\n",
      "        log_handler.setFormatter(logging.Formatter('%(filename)20s: %(message)s'))\r\n",
      "        logging_level = logging.INFO\r\n",
      "    log_handler.setLevel(logging_level)\r\n",
      "    logger.addHandler(log_handler)\r\n",
      "    logger.setLevel(logging_level)\r\n",
      "\r\n",
      "    bert_model = optimize_model(args.input, args.model_type, args.gpu_only, args.num_heads, args.hidden_size, args.sequence_length, args.input_int32, args.float16)\r\n",
      "\r\n",
      "    bert_model.save_model_to_file(args.output)\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "! cat bert_op_scripts/bert_model_optimization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The official script \"bert_model_optimization.py\" has an import \"from BertOnnxModelKeras import BertOnnxModelKeras\".\n",
    "We need to modify our script and remove Keras import as we are using only Tensorflow and official Implementation didn't provided the Keras script and thus We will get an error of \"No module found BertOnnxModelKeras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_model_optimization.py  BertOnnxModel.py\tOnnxModel.py\r\n",
      "bert.onnx\t\t    BertOnnxModelTF.py\t__pycache__\r\n"
     ]
    }
   ],
   "source": [
    "! ls bert_op_scripts/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"bert_op_scripts/bert_model_optimization.py\", line 36, in <module>\r\n",
      "    from BertOnnxModelKeras import BertOnnxModelKeras\r\n",
      "ModuleNotFoundError: No module named 'BertOnnxModelKeras'\r\n"
     ]
    }
   ],
   "source": [
    "# Below are three examples to run bert_model_optimization.py. Choose one according to your needs and adjust --input\n",
    "# --output path names as necessary.\n",
    "\n",
    "# For CPU\n",
    "#!python bert_op_scripts/bert_model_optimization.py --input <bert.onnx> --output <bert_cpu.onnx> --framework tensorflow\n",
    "\n",
    "# # For inferences under NVidia GPU with Tensor Core like V100 and T4\n",
    "# !python bert_op_scripts/bert_model_optimization.py --input <bert.onnx> --output <bert_gpu_fp16.onnx> --framework tensorflow --gpu_only –float16\n",
    "\n",
    "# For inferences under other NVidia GPUs except V100 and T4\n",
    "!python bert_op_scripts/bert_model_optimization.py --input bert.onnx --output bert_gpu_fp32.onnx --framework tensorflow --gpu_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying the Official Script to make it work with tensorflow.\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"bert\" : (BertOnnxModel, \"pytorch\", False),\n",
    "    \"bert_tf\": (BertOnnxModelTF, \"tf2onnx\", True),\n",
    "    \"bert_keras\" : (BertOnnxModelKeras, \"keras2onnx\", False)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4 : Optimizing the ONNX-BERT for GPU with Half-Precision(fp32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BertOnnxModel.py: Fused LayerNormalization count: 0\n",
      "    BertOnnxModel.py: Fused Reshape count:0\n",
      "    BertOnnxModel.py: Fused SkipLayerNormalization count: 0\n",
      "    BertOnnxModel.py: Fused Attention count:0\n",
      "    BertOnnxModel.py: skip embed layer fusion since mask input is not found\n",
      "    BertOnnxModel.py: opset verion: 8\n",
      "        OnnxModel.py: Output model to bert_gpu_fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "!python bert_op_scripts/bert_model_optimization.py --input out/bert.onnx --output bert_gpu_fp32.onnx  --gpu_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5 : Inference on the GPU version of BERT-ONNX using ONNXRUNTIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX GPU Runtime Inference time:  1.1366171836853027\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt  \n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "\n",
    "# Set graph optimization level to ORT_ENABLE_EXTENDED to enable bert optimization.\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "session = rt.InferenceSession(\"out/bert_gpu_fp32.onnx\", sess_options)\n",
    "\n",
    "# evaluate the model\n",
    "# Generate dummy inputs to the model. Adjust if neccessary\n",
    "inputs = {\n",
    "    'input_ids:0':   np.random.randint(0, 256, size=[1, 256], dtype=np.int64), # list of numerical ids for the tokenised text\n",
    "    'segment_ids:0': np.ones(shape=[1, 256], dtype=np.int64),        # dummy list of ones\n",
    "    'input_mask:0':  np.ones(shape=[1, 256], dtype=np.int64),        # dummy list of ones\n",
    "    'unique_ids_raw_output___9:0': np.arange(0, 256, dtype=np.int64)\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "# Run the optimized model with inputs\n",
    "output_names = ['unstack:1', 'unstack:0', 'unique_ids:0']\n",
    "res = session.run(output_names, inputs) \n",
    "end = time.time()\n",
    "gpu_time = end - start\n",
    "print(\"ONNX GPU Runtime Inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6 : Optimizing the ONNX-BERT for CPU(x86)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    BertOnnxModel.py: Fused LayerNormalization count: 0\r\n",
      "    BertOnnxModel.py: Fused Reshape count:0\r\n",
      "    BertOnnxModel.py: Fused SkipLayerNormalization count: 0\r\n",
      "    BertOnnxModel.py: Fused Attention count:0\r\n",
      "    BertOnnxModel.py: skip embed layer fusion since mask input is not found\r\n",
      "    BertOnnxModel.py: opset verion: 8\r\n",
      "        OnnxModel.py: Output model to bert_cpu_only.onnx\r\n"
     ]
    }
   ],
   "source": [
    "#Doing inference only on CPU\n",
    "!python bert_op_scripts/bert_model_optimization.py --input out/bert.onnx --output bert_cpu_only.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7 : Inference on the CPU version of ONNX-BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
      "\u001b[K     |████████████████████████████████| 753.4MB 6.9kB/s  eta 0:00:01��        | 562.6MB 51.1MB/s eta 0:00:04     |████████████████████████████    | 659.9MB 58.8MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU : 1\n",
      "Name of the GPU Tesla K80\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f'Number of GPU : {torch.cuda.device_count()}')\n",
    "print(f'Name of the GPU {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX CPU Runtime Inference time:  1.2086129188537598\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as rt  \n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "\n",
    "# Set graph optimization level to ORT_ENABLE_EXTENDED to enable bert optimization.\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "session = rt.InferenceSession(\"bert_cpu_only.onnx\", sess_options)\n",
    "\n",
    "# evaluate the model\n",
    "# Generate dummy inputs to the model. Adjust if neccessary\n",
    "inputs = {\n",
    "    'input_ids:0':   np.random.randint(0, 256, size=[1, 256], dtype=np.int64), # list of numerical ids for the tokenised text\n",
    "    'segment_ids:0': np.ones(shape=[1, 256], dtype=np.int64),        # dummy list of ones\n",
    "    'input_mask:0':  np.ones(shape=[1, 256], dtype=np.int64),        # dummy list of ones\n",
    "    'unique_ids_raw_output___9:0': np.arange(0, 256, dtype=np.int64)\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "# Run the optimized model with inputs\n",
    "output_names = ['unstack:1', 'unstack:0', 'unique_ids:0']\n",
    "res = session.run(output_names, inputs) \n",
    "end = time.time()\n",
    "cpu_time = end - start\n",
    "print(\"ONNX CPU Runtime Inference time: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion -\n",
    "I used the following configuration for this Project- \n",
    "- Framework - Tensorflow / Sagemaker\n",
    "- EC2 Instance - P2.xlarge\n",
    "- GPU - K80(11GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX GPU Inference time is : 1.1366171836853027\n",
      "ONNX CPU Inferene time is : 1.2086129188537598\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After Optimizing the ONNX Model for CPU and GPU which is also Framework Independent we reduced the Inference time.\n",
    "'''\n",
    "print(f'ONNX GPU Inference time is : {gpu_time}')\n",
    "print(f'ONNX CPU Inferene time is : {cpu_time}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
